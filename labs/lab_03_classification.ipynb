{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 â€’  Classification\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 23.04.2021  \n",
    "**Due date:** 07.05.2021 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as preparation for the final project, which is a practical project which ties together the topics of the course. \n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook !** rerun the notebook from scratch `Kernel` > `Restart & Run All`\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant data\n",
    "We first need to extract the `lab-03-data.tar.gz` archive.\n",
    "To this end, we use the [tarfile] module from the Python standard library.\n",
    "\n",
    "[tarfile]: https://docs.python.org/3.6/library/tarfile.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tarfile\n",
    "import os\n",
    "from sympy import Symbol\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1\n",
    "In this part, we will study classification based on the data available in the Matlab file `classification.mat` that you will under `lab-03-data/part1`.\n",
    "There are 3 data sets in this file, each one being a training set for a given class.\n",
    "They are contained in variables `a`, `b` and `c`.\n",
    "\n",
    "**Note**: we can load Matlab files using the [scipy.io] module.\n",
    "\n",
    "[scipy.io]: https://docs.scipy.org/doc/scipy/reference/io.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (200, 2) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "\n",
    "data_part1_path = os.path.join(data_base_path, data_folder, 'part1', 'classification.mat')\n",
    "matfile = scipy.io.loadmat(data_part1_path)\n",
    "a = matfile['a']\n",
    "b = matfile['b']\n",
    "c = matfile['c']\n",
    "\n",
    "print(a.shape, b.shape, c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Bayes method (5 pts)\n",
    "Using the Bayes method, give the expression of the separation curves between those three classes.\n",
    "Do reasonable hypotheses about the distributions of those classes and estimate the corresponding parameters based on the given training sets.\n",
    "Draw those curves on a plot, together with the training data. Note that you do not need to solve the analytical expression. You can simply evaluate the function for each point on the plane to end up with a segmentation map.\n",
    "For simplicity reasons, round the estimated parameters to the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add your implementation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Mahalanobis distance (5 pts)\n",
    "For classes `a` and `b`, give the expression of the Mahalanobis distance used to classify a point in class `a` or `b`, and verify the obtained classification, in comparison with the \"complete\" Bayes classification, for a few points of the plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix of feature vector of class a =  [[10.  0.]\n",
      " [ 0.  1.]]\n",
      "Covariance matrix of feature vector of class b =  [[10.  0.]\n",
      " [ 0.  1.]]\n",
      "Mean vector of class a =  [[-0.]\n",
      " [ 0.]]\n",
      "Mean vector of class b =  [[3.]\n",
      " [3.]]\n",
      "[[ 2 14]\n",
      " [ 7  2]\n",
      " [ 2  8]\n",
      " [ 2  1]\n",
      " [ 4  9]\n",
      " [14 13]\n",
      " [ 3  7]\n",
      " [13  1]\n",
      " [13  6]\n",
      " [ 0  8]]\n",
      "['b', 'b', 'b', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "# This is from the part that Nikitas made:\n",
    "mu_a = np.around(np.array([np.mean(a[:,0]),np.mean(a[:,1])]).reshape(2,1), 0)\n",
    "mu_b = np.around(np.array([np.mean(b[:,0]),np.mean(b[:,1])]).reshape(2,1), 0)\n",
    "\n",
    "cov_a = np.around(np.cov(a[:,0], a[:,1]), 0)\n",
    "cov_b = np.around(np.cov(b[:,0], b[:,1]), 0)\n",
    "cov_a_inv = np.linalg.inv(cov_a)\n",
    "cov_b_inv = np.linalg.inv(cov_b)\n",
    "\n",
    "print(f'Mean vector of class a = {mu_a}')\n",
    "print(f'Mean vector of class b = {mu_b}')\n",
    "print('Covariance matrix of feature vector of class a = ', cov_a)\n",
    "print('Covariance matrix of feature vector of class b = ', cov_b)\n",
    "\n",
    "# We now calculate the distance with regards to mu_a and cov_a for each row in the a and b arrays\n",
    "# as well as the distance with regards to mu_b and cov_b  for each row in the a and b arrays\n",
    "# which ever distance is smaller will determine which class the point is assigned to\n",
    "\n",
    "# lets create a bunch of random points on the plane which can range between 0 and 15\n",
    "test_vector = np.random.randint(15, size=(10,2))\n",
    "result_vector = []\n",
    "\n",
    "for row in test_vector:\n",
    "\n",
    "    distance_to_a = distance.mahalanobis(row , mu_a, cov_a_inv)\n",
    "    distance_to_b = distance.mahalanobis(row , mu_b, cov_b_inv)\n",
    "\n",
    "    if distance_to_a < distance_to_b:\n",
    "        result_vector.append('a')\n",
    "    elif distance_to_a > distance_to_b: result_vector.append('b')\n",
    "    else: result_vector.append('equalSize')\n",
    "\n",
    "print(test_vector)\n",
    "print(result_vector)\n",
    "\n",
    "# Add your implementation and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2\n",
    "In this part, we aim to classify digits using the complete version of MNIST digits dataset.\n",
    "The dataset consists of 60'000 training images and 10'000 test images of handwritten digits.\n",
    "Each image has size 28x28, and has assigned a label from zero to nine, denoting the digits value.\n",
    "Given this data, your task is to construct a Multilayer Perceptron (MLP) for supervised training and classification and evaluate it on the test images.\n",
    "\n",
    "Download the MNIST dataset (all 4 files) from http://yann.lecun.com/exdb/mnist/ under `lab-03-data/part2`.\n",
    "You can then use the script provided below to extract and load training and testing images in Python. \n",
    "\n",
    "**! Warning**: When the lab was created the official MNIST repo was down, if it is still the case please use https://github.com/mkolod/MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset loading\n",
    "Here we first declare the methods `extract_data` and `extract_labels` so that we can reuse them later in the code.\n",
    "Then we extract both the data and corresponding labels, and plot randomly some images and corresponding labels of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(filename, image_shape, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(np.prod(image_shape) * image_number)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(image_number, image_shape[0], image_shape[1])\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, image_number):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * image_number)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\lab-03-data\\\\part2\\\\train-images-idx3-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-37-c6f3776db560>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[0mtest_labels_path\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_part2_folder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m't10k-labels-idx1-ubyte.gz'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 12\u001B[1;33m \u001B[0mtrain_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_images_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_set_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     13\u001B[0m \u001B[0mtest_images\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest_images_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_set_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[0mtrain_labels\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mextract_labels\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_labels_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_set_size\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-36-d453ae7ff133>\u001B[0m in \u001B[0;36mextract_data\u001B[1;34m(filename, image_shape, image_number)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mextract_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_shape\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mimage_number\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0mgzip\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mbytestream\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m         \u001B[0mbytestream\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m16\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m         \u001B[0mbuf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbytestream\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprod\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage_shape\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mimage_number\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\gzip.py\u001B[0m in \u001B[0;36mopen\u001B[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001B[0m\n\u001B[0;32m     56\u001B[0m     \u001B[0mgz_mode\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"t\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     57\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbytes\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mPathLike\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 58\u001B[1;33m         \u001B[0mbinary_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGzipFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgz_mode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcompresslevel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     59\u001B[0m     \u001B[1;32melif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"read\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"write\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[0mbinary_file\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mGzipFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgz_mode\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcompresslevel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfilename\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\gzip.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001B[0m\n\u001B[0;32m    171\u001B[0m             \u001B[0mmode\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;34m'b'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    172\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfileobj\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 173\u001B[1;33m             \u001B[0mfileobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmyfileobj\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbuiltins\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m \u001B[1;32mor\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    174\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfilename\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    175\u001B[0m             \u001B[0mfilename\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfileobj\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'name'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m''\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '..\\\\data\\\\lab-03-data\\\\part2\\\\train-images-idx3-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "image_shape = (28, 28)\n",
    "train_set_size = 60000\n",
    "test_set_size = 10000\n",
    "\n",
    "data_part2_folder = os.path.join(data_base_path, data_folder, 'part2')\n",
    "\n",
    "train_images_path = os.path.join(data_part2_folder, 'train-images-idx3-ubyte.gz')\n",
    "train_labels_path = os.path.join(data_part2_folder, 'train-labels-idx1-ubyte.gz')\n",
    "test_images_path = os.path.join(data_part2_folder, 't10k-images-idx3-ubyte.gz')\n",
    "test_labels_path = os.path.join(data_part2_folder, 't10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "train_images = extract_data(train_images_path, image_shape, train_set_size)\n",
    "test_images = extract_data(test_images_path, image_shape, test_set_size)\n",
    "train_labels = extract_labels(train_labels_path, train_set_size)\n",
    "test_labels = extract_labels(test_labels_path, test_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prng = np.random.RandomState(seed=123456789)  # seed to always re-draw the same distribution\n",
    "plt_ind = prng.randint(low=0, high=train_set_size, size=10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 10, figsize=(12, 3))\n",
    "for ax, im, lb in zip(axes, train_images[plt_ind], train_labels[plt_ind]):\n",
    "    ax.imshow(im, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 MLP (10 pts)\n",
    "\n",
    "To create an MLP you are free to choose any library.\n",
    "In case you don't have any preferences, we encourage you to use the [scikit-learn] package; it is a simple, efficient and free tool for data analysis and machine learning.\n",
    "In this [link][sklearn-example], you can find a basic example to see how to create and train an MLP using [scikit-learn].\n",
    "Your network should have the following properties:\n",
    "* Input `x`: 784-dimensional (i.e. 784 visible units representing the flattened 28x28 pixel images).\n",
    "* 100 hidden units `h`.\n",
    "* 10 output units `y`, i.e. the labels, with a value close to one in the i-th class representing a high probability of the input representing the digit `i`.\n",
    "\n",
    "If you need additional examples you can borrow some code from image classification tutorials.\n",
    "However, we recommend that you construct a minimal version of the network on your own to gain better insights.\n",
    "\n",
    "[scikit-learn]: http://scikit-learn.org/stable/index.html\n",
    "[sklearn-example]: http://scikit-learn.org/stable/modules/neural_networks_supervised.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your implementation and discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}